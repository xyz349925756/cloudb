参考和汇总整理的前面几版的问题和基础，此版本是二进制的定格版完全实用于生产环境，目前采用最新版本

`流程图示例`

```flow
st=>start: 规划资源
op=>operation: 准备工作
cond=>condition: 是否准备好
e=>end: 结束

st->op->cond
cond(yes)->e
cond(no)->op
```



>  lvs+keepalive+nginx 实现apiserver 的集群和高可用
>
>  https://github.com/xyz349925756/kubernetes   本文常用的脚本，下载之后自己修改

<a name="一览表">一览表</a>

| 名称\主机名        |  master01   |  master02   |  master03   |   node01    |   node02    |
| ------------------ | :---------: | :---------: | :---------: | :---------: | :---------: |
| IP地址^IPv4^       | 172.16.0.30 | 172.16.0.31 | 172.16.0.32 | 172.16.0.35 | 172.16.0.36 |
| APIserver          |     [√]     |     [√]     |     [√]     |     [x]     |     [x]     |
| Controller-manager |     [√]     |     [√]     |     [√]     |     [x]     |     [x]     |
| Scheduler          |     [√]     |     [√]     |     [√]     |     [x]     |     [x]     |
| Kubelet            |     [√]     |     [√]     |     [√]     |     [√]     |     [√]     |
| Kube-proxy         |     [√]     |     [√]     |     [√]     |     [√]     |     [√]     |
| Harbor             |     [√]     |     [x]     |     [x]     |     [x]     |     [x]     |
| Cfssl              |     [√]     |     [x]     |     [x]     |     [x]     |     [x]     |

# 系统优化及个性化设置

### **网卡设置**

```BASH
[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
BOOTPROTO=static
ONBOOT=yes
IPADDR=172.16.0.19
NETMASK=255.255.255.0
GATEWAY=172.16.0.1

设置DNS
[root@localhost ~]# echo -e "nameserver 114.114.114.114\nnameserver 172.16.0.1" >>/etc/resolv.conf 
[root@localhost ~]# cat /etc/resolv.conf 
# Generated by NetworkManager
nameserver 114.114.114.114
nameserver 172.16.0.1
[root@localhost ~]# ping -w 2 -c 3 baidu.com
PING baidu.com (39.156.69.79) 56(84) bytes of data.
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=1 ttl=53 time=55.6 ms
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=2 ttl=53 time=59.8 ms

--- baidu.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1003ms
rtt min/avg/max/mdev = 55.653/57.763/59.873/2.110 ms

```

### **克隆虚拟机环境之后优化脚本**

```BASH
#!/bin/bash
#----------------------------------------------
# Author        : 349925756
# Email         : 349925756@qq.com
# Last modified : 2021-06-08 21:31
# Filename      : uuid.sh
# Description   : 
# Version       : 1.1 
#----------------------------------------------
#修改网卡UUID IP地址 主机名，关闭防火墙，selinux 安装常用工具关闭swap 导入kubectl tab补全
#uuid  ip
path_eth0="/etc/sysconfig/network-scripts/ifcfg-eth0"
sed -i "/UUID/c UUID=$(uuidgen)" $path_eth0   
sed -i "s/$1/$2/g" $path_eth0
echo "$3" >/etc/hostname
systemctl stop firewalld && systemctl disable firewalld
sed -i "s/SELINUX=.*/SELINUX=disabled/g" /etc/selinux/config
\cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
systemctl enable chronyd
sed -ri 's/.*swap.*/#&/' /etc/fstab 
reboot

使用方法：sh /$path/$filename.sh  原IP地址  目标IP地址  主机名
例：sh uuid.sh 120 30 master01
```

### 常用软件

```BASH
yum install -y vim wget net-tools tree nmap dos2unix lrzsz nc lsof tcpdump htop iftop iotop sysstat nethogs git iptables conntrack ipvsadm ipset jq sysstat libseccomp
```



### 镜像源更换

```BASH
阿里源
http://mirrors.aliyun.com/repo/
[root@localhost ~]# cp /etc/yum.repos.d/CentOS-Base.repo{,.bak}
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
[root@localhost ~]# yum clean all
[root@localhost ~]# yum makecache

epel 源  
[root@localhost ~]# yum list|grep epel
[root@localhost ~]# yum install grep epel-release.noarch  -y
[root@localhost ~]# yum clean all
[root@localhost ~]# yum makecache

[root@localhost ~]# yum repolist enabled    #查看启用的仓库
[root@localhost ~]# yum repolist all   #查看所有的仓库

yum update

#kubernetes 镜像源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
# yum install -y --nogpgcheck kubelet kubeadm kubectl  # 由于官网未开放同步方式, 可能会有索引gpg检查失败的情况, 

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF
```

### 个性化设置

```BASH
PS1='[\[\e[31;40m\]\u\[\e[33;40m\]@\[\e[34;40m\]\h \[\e[33;40m\]\w\[\e[0m\]]\$ '
[root@localhost ~]# source /etc/bashrc
```

上传vimrc文件到~

```BASH
[root@localhost ~]# mv vimrc.2222 .vimrc
[root@localhost ~]# cat .vimrc    #个性化设置vim 的
"设置行号"
"set nu"
"自动语法高亮"
syntax on
"自动缩进"
"set autoindent"
"关闭兼容模式"
set nocompatible
"激活鼠标"
"set mouse=c"
"开启语法"
syntax enable
"tab缩进4个空格"
"set tabstop=4"
"设定<< >>移动宽度4"
set shiftwidth=4
"自动缩进"
set ai
"智能缩进"
set si
"显示标尺"
set ruler
"显示匹配的[]{}"
set showmatch
"编码设置"
set encoding=utf-8
set fileencodings=utf-t
set termencoding=utf-8
"开启新行时使用智能自动缩进"
set smartindent
set cin
set showmatch
"背景色"
"set background=dark"
"设置光标下划线"
set cursorline

map <F10> : set paste <cr>
map <F11> : set nopaste <cr>

autocmd BufNewFile *.sh exec ":call AddUsr()"
map <F7> ms:call AddTitle()<cr>'s

function AddAuthor()
        let n=1
        while n < 5
                let line = getline(n)
                if line =~'^\s*\*\s*\S*Last\s*modified\s*:\s*\S*.*$'
                        call UpdateTitle()
                        return
                endif
                let n = n + 1
        endwhile
        call AddTitle()
endfunction

function UpdateTitle()
        normal m'
        execute '/* Last modified\s*:/s@:.*$@\=strftime(": %Y-%m-%d %H:%M")@'
        normal "
        normal mk
        execute '/* Filename\s*:/s@:.*$@\=": ".expand("%:t")@'
        execute "noh"
        normal 'k
        echohl WarningMsg | echo "Successful in updating the copy right." | echohl None
endfunction

function AddTitle()
        call append(0,"#!/bin/bash")
        call append(1,"#----------------------------------------------")
        call append(2,"# Author        : 349925756")
        call append(3,"# Email         : 349925756@qq.com")
        call append(4,"# Last modified : ".strftime("%Y-%m-%d %H:%M"))
        call append(5,"# Filename      : ".expand("%:t"))
        call append(6,"# Description   : ")
        call append(7,"# Version       : 1.1 ")
        call append(8,"#----------------------------------------------")
        call append(9," ")
    	call append(10,"#Notes:  ")
        echohl WarningMsg | echo "Successful in adding the copyright." | echohl None

endfunction

function AddUsr()
        call append(0,"#!/bin/bash")
endfunction



F10是 设置无格式粘贴的
F7 是一键加入作者信息注释
```

### 内核升级

https://www.kernel.org/

```BASH
[root@localhost ~]# uname -sr
Linux 3.10.0-1160.31.1.el7.x86_64
```

大多数现代发行版提供了一种使用 [yum 等包管理系统](http://www.tecmint.com/20-linux-yum-yellowdog-updater-modified-commands-for-package-mangement/)和官方支持的仓库升级内核的方法。

但是，这只会升级内核到仓库中可用的最新版本 - 而不是在 https://www.kernel.org/ 中可用的最新版本。不幸的是，Red Hat 只允许使用前者升级内核。

与 Red Hat 不同，CentOS 允许使用 ELRepo，这是一个第三方仓库，可以将内核升级到最新版本。

要在 CentOS 7 上启用 ELRepo 仓库      官网：http://elrepo.org/tiki/HomePage

```BASH
[root@localhost ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
[root@localhost ~]# yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y

列出可用内核包
[root@localhost ~]# yum --disablerepo="*" --enablerepo="elrepo-kernel" list available
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * elrepo-kernel: mirror-hk.koddos.net
elrepo-kernel                                                                                                              | 3.0 kB  00:00:00     
elrepo-kernel/primary_db                                                                                                   | 2.0 MB  00:00:01     
Available Packages
kernel-lt.x86_64                      5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-devel.x86_64                5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-doc.noarch                  5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-headers.x86_64              5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-tools.x86_64                5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-tools-libs.x86_64           5.4.131-1.el7.elrepo        elrepo-kernel
kernel-lt-tools-libs-devel.x86_64     5.4.131-1.el7.elrepo        elrepo-kernel
kernel-ml.x86_64                      5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-devel.x86_64                5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-doc.noarch                  5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-headers.x86_64              5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-tools.x86_64                5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-tools-libs.x86_64           5.13.1-1.el7.elrepo         elrepo-kernel
kernel-ml-tools-libs-devel.x86_64     5.13.1-1.el7.elrepo         elrepo-kernel
perf.x86_64                           5.13.1-1.el7.elrepo         elrepo-kernel
python-perf.x86_64                    5.13.1-1.el7.elrepo         elrepo-kernel

mainline    ml   主线包
longterm    lt    长期维护版  
stable               稳定版
linux-next 、 snapshot  快照

安装最新内核
[root@localhost ~]# yum --enablerepo=elrepo-kernel install kernel-ml

reboot
手动选择最新的内核启动
[root@localhost ~]# uname -sr
Linux 5.13.1-1.el7.elrepo.x86_64

设置GRUB 默认的内核
为了让新安装的内核成为默认启动选项，你需要如下修改 GRUB 配置：
打开并编辑 /etc/default/grub 并设置 GRUB_DEFAULT=0。意思是 GRUB 初始化页面的第一个内核将作为默认内核。
[root@localhost ~]# cp /etc/default/grub{,.bak} 
[root@localhost ~]# vim /etc/default/grub 
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=0                                                                                                                                    
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root biosdevname=0 net.ifnames=0 rhgb quiet"
GRUB_DISABLE_RECOVERY="true"

重新创建内核配置
[root@localhost ~]# grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-5.13.1-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-5.13.1-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-1160.31.1.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-1160.31.1.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-1160.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-1160.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-bbc3da5ee5204d3aaf3fa07716781ee1
Found initrd image: /boot/initramfs-0-rescue-bbc3da5ee5204d3aaf3fa07716781ee1.img
done

```

### 删除旧内核

```BASH
[root@localhost ~]# rpm -qa |grep kernel
kernel-ml-5.13.1-1.el7.elrepo.x86_64
kernel-3.10.0-1160.el7.x86_64
kernel-tools-libs-3.10.0-1160.31.1.el7.x86_64
kernel-3.10.0-1160.31.1.el7.x86_64
kernel-tools-3.10.0-1160.31.1.el7.x86_64
方法1：
yum remove kernel-3.10.0-1160.el7.x86_64 \
yum remove kernel-tools-libs-3.10.0-1160.31.1.el7.x86_64 \
yum remove kernel-3.10.0-1160.31.1.el7.x86_64
reboot

 方法2：
yum install yum-utils -y    #这里我选择方法二，后面这个用得到的很多
package-cleanup --oldkernels
#如果安装的内核不多于 3 个，yum-utils 工具不会删除任何一个。只有在安装的内核大于 3 个时，才会自动删除旧内核。
---------------------------------------------------------------------
[root@localhost ~]# yum remove `rpm -qa |grep kernel|grep -v 5.14`
[root@localhost ~]# rpm -qa |grep kernel
kernel-ml-5.14.13-1.el7.elrepo.x86_64

```

### 给grub 加密

```BASH
[root@localhost ~]# grub2-setpassword 
Enter password: 
Confirm password: 

目的就是防止他人single 登陆  （单用户登陆）
```

### 内核参数

```BASH
[root@master01 ~]# cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
[root@master01 ~]# sysctl -p /etc/sysctl.d/k8s.conf
```

### 加载ipvsadm

```BASH
# ipvsadm
[root@localhost ~]# lsmod | grep ip_vs
ip_vs                 163840  0 
nf_conntrack          155648  3 xt_conntrack,nf_nat,ip_vs
nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
libcrc32c              16384  4 nf_conntrack,nf_nat,xfs,ip_vs
```



### 登陆界面 

不要修改

```bash
[root@localhost ~]# vim /etc/issue
+--------------------------------------------------------------------------+
                              .       .
                             / `.   .' "
                     .---.  <    > <    >  .---.
                     |    \  \ - ~ ~ - /  /    |
         _____          ..-~             ~-..-~
        |     |   \~~~\.'                    `./~~~/
       ---------   \__/                        \__/
      .'  O    \     /               /       \  "
     (_____,    `._.'               |         }  \/~~~/
      `----.          /       }     |        /    \__/
            `-.      |       /      |       /      `. ,~~|
                ~-.__|      /_ - ~ ^|      /- _      `..-'
                     |     /        |     /     ~-.     `-. _  _  _
                     |_____|        |_____|         ~ - . _ _ _ _ _>


+--------------------------------------------------------------------------+

               IP:\4               Tel:18183510256

+--------------------------------------------------------------------------+               
登录系统提示
[root@localhost ~]# chmod +x /etc/rc.d/rc.local
[root@localhost ~]# vim /etc/rc.d/rc.local 
echo "Hostname: $(hostname -s)" >/etc/motd
echo "IP      : $(hostname -i)" >>/etc/motd  


[root@localhost ~]# echo -e "\033[41;37m Hostname : \033[0m \033[32m $(hostname -s) \033[0m"
 Hostname :   localhost 

[root@localhost ~]# echo -e "\033[34m Hostname : \033[0m \033[43;37m $(hostname -s) \033[0m"
 Hostname :   localhost 

```

![image-20211017211949416](kubernetes二进制部署.assets/image-20211017211949416.png)

完整的

```sh
[root@localhost ~]# vim /etc/rc.d/rc.local 
echo -e "\033[41;37m Hostname : \033[0m \033[32m $(hostname -s) \033[0m" >/etc/motd
echo -e "\033[43;37m IP  Addr : \033[0m \033[32m $(hostname -i) \033[0m"  >>/etc/motd
echo -e "\033[44;37m DataTime : \033[0m \033[32m $(date +%F_%H:%M:%S) \033[0m" >>/etc/motd       
```

![image-20211017213048533](kubernetes二进制部署.assets/image-20211017213048533.png)

```sh
echo -e "\033[41;37m Hostname : \033[0m \033[31m $(hostname -s) \033[0m" >/etc/motd
echo -e "\033[43;37m IP  Addr : \033[0m \033[33m $(hostname -i) \033[0m"  >>/etc/motd
echo -e "\033[44;37m DataTime : \033[0m \033[34m $(date +%F_%H:%M:%S) \033[0m" >>/etc/motd     
```

![image-20211017213243369](kubernetes二进制部署.assets/image-20211017213243369.png)

https://codechina.csdn.net/mirrors/abcfy2/motd?utm_source=csdn_github_accelerator 更多图案

grub加密

```sh
[root@localhost ~]# grub2-setpassword 
```




## 命令自动补全

kubernetes

```BASH
echo 'source /usr/share/bash-completion/bash_completion' >> ~/.bashrc
echo 'source <(kubectl completion bash)' >> ~/.bashrc
```

docker

https://github.com/xyz349925756/kubernetes/tree/main/tab_completion

```BASH
yum -y install bash-completion bash-completion-extras
source /etc/profile.d/bash_completion.sh

[root@master01 /opt/yaml]# cp /server/docker* /usr/share/bash-completion/completions/

[root@master01 ~]# for i in master{01..03} node{01,02}; do scp /usr/share/bash-completion/completions/docker* $i:/usr/share/bash-completion/completions/ ;done
docker                                                               100%  114KB  69.7MB/s   00:00    
docker-compose                                                       100%   12KB  14.3MB/s   00:00    
docker                                                               100%  114KB  38.8MB/s   00:00    
docker-compose                                                       100%   12KB   8.5MB/s   00:00    
docker                                                               100%  114KB   4.1MB/s   00:00    
docker-compose                                                       100%   12KB   6.1MB/s   00:00    
docker                                                               100%  114KB   7.2MB/s   00:00    
docker-compose                                                       100%   12KB   9.7MB/s   00:00    
docker                                                               100%  114KB   9.5MB/s   00:00    
docker-compose                                                       100%   12KB   6.1MB/s   00:00    

reboot
```

按照上面[一览表](#一览表)

因为kubeadm 没有挑战就不赘述了。

# 二进制软件准备

| **名称**           | **下载页面**                                                 |
| ------------------ | ------------------------------------------------------------ |
| **Centos**         | https://www.centos.org/download/    #系统下载                |
| **Docker**         | https://download.docker.com/linux/static/stable/x86_64/      |
| **docker-compose** | https://github.com/docker/compose/releases/                  |
| **Kubernetes**     | https://github.com/kubernetes                                |
| **Calico**         | https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises |
| **Coredns**        | https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/coredns |
| **Dashboard**      | https://github.com/kubernetes/dashboard/releases             |
| **cfssl**          | https://github.com/cloudflare/cfssl/releases                 |
| **Etcd**           | https://github.com/etcd-io/etcd/releases                     |
| **CNI**            | https://github.com/containernetworking/plugins/releases      |
| **calicoctl**      | https://github.com/projectcalico/calicoctl/releases          |
| **harbor**         | https://github.com/goharbor/harbor/releases                  |

## 创建软件统一存放目录

```BASH
[root@master01 ~]# mkdir -p /server/soft
[root@master01 ~]# ll /server/soft/
[root@master01 ~]# cd /server/soft/
[root@master01 /server/soft]# chmod +x calicoctl-linux-amd64 cfssl* docker-compose-Linux-x86_64 
[root@master01 /server/soft]# ll
total 565248
-rwxr-xr-x 1 root root  40022016 May 26 03:00 calicoctl-linux-amd64
-rw-r--r-- 1 root root     24800 Jul 13 16:18 calico-etcd.yaml
-rwxr-xr-x 1 root root  16377936 Jul 13 16:19 cfssl_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  13245520 Jul 13 16:18 cfssl-certinfo_1.6.0_linux_amd64
-rwxr-xr-x 1 root root  10892112 Jul 13 16:18 cfssljson_1.6.0_linux_amd64
-rw-r--r-- 1 root root  39771622 Jul 13 16:19 cni-plugins-linux-amd64-v0.9.1.tgz
-rw-r--r-- 1 root root      3809 Jul 13 16:19 coredns-1.8.yaml
-rw-r--r-- 1 root root      4606 Jul 13 16:19 coredns.yaml
-rw-r--r-- 1 root root  69725147 Jul 13 16:19 docker-20.10.7.tgz
-rwxr-xr-x 1 root root  12737304 Jul 13 16:19 docker-compose-Linux-x86_64
-rw-r--r-- 1 root root  19389988 Jul 13 16:19 etcd-v3.5.0-linux-amd64.tar.gz
-rw-r--r-- 1 root root 342258563 Jul 13 16:19 kubernetes-server-linux-amd64_1.21.2.tar.gz

#移动命令到/usr/local/bin
[root@master01 /server/soft]# mv calicoctl-linux-amd64 /usr/local/bin/calicoctl
[root@master01 /server/soft]# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose
[root@master01 /server/soft]# mv cfssl_1.6.0_linux_amd64 /usr/local/bin/cfssl
[root@master01 /server/soft]# mv cfssljson_1.6.0_linux_amd64 /usr/local/bin/cfssljson
[root@master01 /server/soft]# mv cfssl-certinfo_1.6.0_linux_amd64 /usr/local/bin/cfssl-certinfo
```

### 创建控制端文件目录

```BASH
mkdir -p  /opt/{etcd,kubernetes}/{cfg,ssl,logs} \
               ~/.kube \
               /opt/cni/bin \
               /etc/cni/net.d
               
mkdir -p   ~/tls \  #master01才需要其他不需要
```

### 客户端

```BASH
mkdir -p /opt/kubernetes/{cfg,ssl,logs} \
               /opt/cni/bin \
               /etc/cni/net.d
```

  

| 目录路径             | 说明                    |
| -------------------- | ----------------------- |
| ~/tls                | TLS证书创建目录         |
| /opt/etcd/cfg        | etcd配置文件            |
| /opt/etcd/ssl        | etcd证书文件            |
| /opt/kubernetes/ssl  | kubernetes 证书文件     |
| /opt/kubernetes/cfg  | kubernetes 配置文件     |
| /opt/kubernetes/logs | kubernetes 日志文件     |
| /opt/cni/bin         | CNI插件命令             |
| /etc/cni/net.d       | 存放cni桥接网卡配置文件 |

##  创建互信

  ```BASH
  #安装免输入密码
  yum install sshpass -y
  #不输入yes /no的确认
  "-o StrictHostKeyChecking=no"
  ```

### 添加hosts

```BASH
echo -e "172.16.0.30 master01\n172.16.0.31 master02\n172.16.0.32 master03\n172.16.0.35 node01\n172.16.0.36 node02\n" >>/etc/hosts

[root@master01 /server/soft]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
172.16.0.30 master01
172.16.0.31 master02
172.16.0.32 master03
172.16.0.35 node01
172.16.0.36 node02
```

###  互信脚本

```BASH
[root@master01 ~]# ssh-keygen -t rsa
[root@master01 ~]# vim push_ssh_key.sh
for h in master{01..03} node{01,02};
do
  echo "---------------------------HostName: $h Pub-Key start ------------------------------" ;
  sshpass -p******** ssh-copy-id -i  /root/.ssh/id_rsa.pub $h "-o StrictHostKeyChecking=no";
  echo -e "HostName	: $h Send Successfull" ;
  echo "---------------------------HostName: $h  Pub-Key end ------------------------------";
  echo -e " \n" ;
done

#这里是master01传到所有节点，其他节点需要传也需要这么做。上面的密码改成自己的，也可以设置为变量

#检查ssh
[root@master01 ~]# vim check_ssh.sh 
#!/bin/bash
CMD=$1
for h in master{01..03} node{01,02};
do
  echo "--------------------HostName: $h Check-----------------------------";
  ssh $h $CMD;
  echo "";
done

[root@master01 ~]# sh check_ssh.sh "hostname -i"
--------------------HostName: master01 Check-----------------------------
172.16.0.30
--------------------HostName: master02 Check-----------------------------
172.16.0.31
--------------------HostName: master03 Check-----------------------------
172.16.0.32
--------------------HostName: node01 Check-----------------------------
172.16.0.35
--------------------HostName: node02 Check-----------------------------
172.16.0.36
```

修改版

```
#!/bin/bash
rpm -qa |grep sshpass 
[ $? -ne 0 ] && yum install -y sshpass
for i in ceph{01..03};
do
   sshpass -p$1 ssh-copy-id -i /root/.ssh/id_rsa.pub ${i} "-o StrictHostKeyChecking=no";
   echo -e "-------------------${i} send successfull---------------------------";
done
```



### 传送文件脚本

```BASH
[root@master01 ~]# vim scp_file_all.sh
for h in master{01..03} node{01,02};
do
  echo "---------------------------File: $h Send start ------------------------------" ;
  scp $1 $h:$2;
  echo -e "File	: $1 Send Successfull" ;
  echo "---------------------------File: $1  Send end ------------------------------";
done

$1 是参数是本地文件或者全路径
$2 是目标路径
```

## 证书机构

### CA


```sh
根证书
cat > ca-csr.json <<EOF
{
    "CN": "CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
   "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming",
            "O": "CA",
            "OU": "System"
        }                                                                                                                    
    ]
}
EOF

[root@master01 ~/tls]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 
ca-key.pem  ca.pem

#ca配置文件，以下的证书都是通过这个证书颁发的
cat > ca-config.json << EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "etcd": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            },
            "kubernetes": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF
```


​    

### ETCD

```BASH
cat > etcd-csr.json <<EOF
{
    "CN": "etcd-ca",
    "hosts": [
        "localhost",
        "127.0.0.1",
        "master01",
        "master02",
        "master03",
        "172.16.0.30",
        "172.16.0.31",
        "172.16.0.32",
        "172.16.0.33",
        "172.16.0.34"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Yunnan",
            "L": "Kunming"
        }
    ]
}
EOF

[root@master01 ~/tls]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd
```

### admin

```BASH
cat > admin-csr.json <<EOF
{
  "CN": "kubernetes-admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

[root@master01 ~/tls]# cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin

```

### kubelet

```BASH
for i in master{01..03} node{01,02};do
cat > ${i}-csr.json <<EOF
{
  "CN": "system:node:${i}",
   "hosts": [
   "localhost",
    "127.0.0.1",
    "172.16.0.30",
    "172.16.0.31",
    "172.16.0.32",
    "172.16.0.33",
    "172.16.0.34",
    "172.16.0.35",
    "172.16.0.36",
    "172.16.0.37",
    "master01",
    "master02",
    "master03",
    "node01",
    "node02"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:nodes",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes ${i}-csr.json | cfssljson -bare ${i}
done
```

### kube-controller-manager

```BASH
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-controller-manager",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```

### kube-scheduler

```BASH
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:kube-scheduler",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```

### kube-apiserver

```BASH
cat > kube-apiserver-csr.json <<EOF
{
  "CN": "kube-apiserver",
  "hosts": [
  "127.0.0.1",
  "10.0.0.1",
  "172.16.0.30",
  "172.16.0.31",
  "172.16.0.32",
  "172.16.0.33",
  "172.16.0.34",
  "172.16.0.35",
  "172.16.0.36",
  "172.16.0.37",
  "master01",
  "master02",
  "master03",
  "node01",
  "node02",
  "kubernetes",
  "kubernetes.default",
  "kubernetes.default.svc",
  "kubernetes.default.svc.cluster",
  "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
```

### kube-proxy

```BASH
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:node-proxy",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

### service-account

```BASH
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Kunming",
      "O": "system:masters",
      "OU": "System",
      "ST": "Yunnan"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes service-account-csr.json | cfssljson -bare service-account
```

### 检查证书信息

```BASH
[root@master01 ~/tls]# for i in `ls *.pem|grep -v key`;do openssl verify -CAfile ca.pem  $i;done
admin.pem: OK
ca.pem: OK
etcd.pem: OK
kube-apiserver.pem: OK
kube-controller-manager.pem: OK
kube-proxy.pem: OK
kube-scheduler.pem: OK
master01.pem: OK
master02.pem: OK
master03.pem: OK
node01.pem: OK
node02.pem: OK
service-account.pem: OK

for i in `ls *.pem|grep -v key`;do openssl x509 -in $i -text -noout |grep CA:;done  #检查是否可以再生成证书许可
```

| 默认 CN                       | 父级 CA                   | O (位于 Subject 中) | 类型           | 主机 (SAN)                                          |
| ----------------------------- | ------------------------- | ------------------- | -------------- | --------------------------------------------------- |
| kube-etcd                     | etcd-ca                   |                     | server, client | `localhost`, `127.0.0.1`                            |
| kube-etcd-peer                | etcd-ca                   |                     | server, client | `<hostname>`, `<Host_IP>`, `localhost`, `127.0.0.1` |
| kube-etcd-healthcheck-client  | etcd-ca                   |                     | client         |                                                     |
| kube-apiserver-etcd-client    | etcd-ca                   | system:masters      | client         |                                                     |
| kube-apiserver                | kubernetes-ca             |                     | server         | `<hostname>`, `<Host_IP>`, `<advertise_IP>`, `[1]`  |
| kube-apiserver-kubelet-client | kubernetes-ca             | system:masters      | client         |                                                     |
| front-proxy-client            | kubernetes-front-proxy-ca |                     | client         |                                                     |

[1]: 用来连接到集群的不同 IP 或 DNS 名 （就像 [kubeadm](https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/) 为负载均衡所使用的固定 IP 或 DNS 名，`kubernetes`、`kubernetes.default`、`kubernetes.default.svc`、 `kubernetes.default.svc.cluster`、`kubernetes.default.svc.cluster.local`）。

证书路径

| 默认 CN                       | 建议的密钥路径               | 建议的证书路径               | 命令                    | 密钥参数                   | 证书参数                                                     |
| ----------------------------- | ---------------------------- | ---------------------------- | ----------------------- | -------------------------- | ------------------------------------------------------------ |
| etcd-ca                       | etcd/ca.key                  | etcd/ca.crt                  | kube-apiserver          |                            | --etcd-cafile                                                |
| kube-apiserver-etcd-client    | apiserver-etcd-client.key    | apiserver-etcd-client.crt    | kube-apiserver          | --etcd-keyfile             | --etcd-certfile                                              |
| kubernetes-ca                 | ca.key                       | ca.crt                       | kube-apiserver          |                            | --client-ca-file                                             |
| kubernetes-ca                 | ca.key                       | ca.crt                       | kube-controller-manager | --cluster-signing-key-file | --client-ca-file, --root-ca-file, --cluster-signing-cert-file |
| kube-apiserver                | apiserver.key                | apiserver.crt                | kube-apiserver          | --tls-private-key-file     | --tls-cert-file                                              |
| kube-apiserver-kubelet-client | apiserver-kubelet-client.key | apiserver-kubelet-client.crt | kube-apiserver          | --kubelet-client-key       | --kubelet-client-certificate                                 |
| front-proxy-ca                | front-proxy-ca.key           | front-proxy-ca.crt           | kube-apiserver          |                            | --requestheader-client-ca-file                               |
| front-proxy-ca                | front-proxy-ca.key           | front-proxy-ca.crt           | kube-controller-manager |                            | --requestheader-client-ca-file                               |
| front-proxy-client            | front-proxy-client.key       | front-proxy-client.crt       | kube-apiserver          | --proxy-client-key-file    | --proxy-client-cert-file                                     |
| etcd-ca                       | etcd/ca.key                  | etcd/ca.crt                  | etcd                    |                            | --trusted-ca-file, --peer-trusted-ca-file                    |
| kube-etcd                     | etcd/server.key              | etcd/server.crt              | etcd                    | --key-file                 | --cert-file                                                  |
| kube-etcd-peer                | etcd/peer.key                | etcd/peer.crt                | etcd                    | --peer-key-file            | --peer-cert-file                                             |
| etcd-ca                       |                              | etcd/ca.crt                  | etcdctl                 |                            | --cacert                                                     |
| kube-etcd-healthcheck-client  | etcd/healthcheck-client.key  | etcd/healthcheck-client.crt  | etcdctl                 | --key                      | --cert                                                       |

注意事项同样适用于服务帐户密钥对：

| 私钥路径 | 公钥路径 | 命令                    | 参数                               |
| -------- | -------- | ----------------------- | ---------------------------------- |
| sa.key   |          | kube-controller-manager | --service-account-private-key-file |
|          | sa.pub   | kube-apiserver          | --service-account-key-file         |

#### 为用户帐户配置证书

你必须手动配置以下管理员帐户和服务帐户：

| 文件名                  | 凭据名称                   | 默认 CN                        | O (位于 Subject 中) |
| ----------------------- | -------------------------- | ------------------------------ | ------------------- |
| admin.conf              | default-admin              | kubernetes-admin               | system:masters      |
| kubelet.conf            | default-auth               | system:node:`<nodeName>`       | system:nodes        |
| controller-manager.conf | default-controller-manager | system:kube-controller-manager |                     |
| scheduler.conf          | default-scheduler          | system:kube-scheduler          |                     |

> **说明：** `kubelet.conf` 中 `<nodeName>` 的值 **必须** 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。 

上面表格为官网默认名称，勿改动！

​              

### 分发证书

```BASH
#client
for i in master{01..03} node{01,02};do echo "+----------${i}----------+";scp ca.pem  ${i}.pem ${i}-key.pem ${i}:/opt/kubernetes/ssl;done
+----------master01----------+
ca.pem                                100% 1281   624.7KB/s   00:00    
master01.pem                          100% 1570   577.6KB/s   00:00    
master01-key.pem                      100% 1675   735.4KB/s   00:00    
+----------master02----------+
ca.pem                                100% 1281   314.3KB/s   00:00    
master02.pem                          100% 1570   570.1KB/s   00:00    
master02-key.pem                      100% 1679   771.8KB/s   00:00    
+----------master03----------+
ca.pem                                100% 1281   635.7KB/s   00:00    
master03.pem                          100% 1570   722.1KB/s   00:00    
master03-key.pem                      100% 1679   111.2KB/s   00:00    
+----------node01----------+
ca.pem                                100% 1281   495.9KB/s   00:00    
node01.pem                            100% 1570   494.5KB/s   00:00    
node01-key.pem                        100% 1679   746.1KB/s   00:00    
+----------node02----------+
ca.pem                                100% 1281   644.6KB/s   00:00    
node02.pem                            100% 1570   534.5KB/s   00:00    
node02-key.pem                        100% 1675   699.6KB/s   00:00 

#server
for i in master{01..03};do echo "+----------${i}----------+";scp ca*.pem kube-apiserver*.pem  service-account*.pem ${i}:/opt/kubernetes/ssl;done
+----------master01----------+
ca-key.pem                            100% 1679   615.0KB/s   00:00  
ca.pem                                100% 1281   660.1KB/s   00:00  
kube-apiserver-key.pem                100% 1679   525.3KB/s   00:00  
kube-apiserver.pem                    100% 1736   518.1KB/s   00:00  
service-account-key.pem               100% 1675   985.2KB/s   00:00  
service-account.pem                   100% 1399    67.3KB/s   00:00  
+----------master02----------+
ca-key.pem                            100% 1679   128.3KB/s   00:00  
ca.pem                                100% 1281   764.0KB/s   00:00  
kube-apiserver-key.pem                100% 1679   837.8KB/s   00:00  
kube-apiserver.pem                    100% 1736   908.2KB/s   00:00  
service-account-key.pem               100% 1675   831.3KB/s   00:00  
service-account.pem                   100% 1399   573.6KB/s   00:00  
+----------master03----------+
ca-key.pem                            100% 1679   362.5KB/s   00:00  
ca.pem                                100% 1281   285.3KB/s   00:00  
kube-apiserver-key.pem                100% 1679   657.7KB/s   00:00  
kube-apiserver.pem                    100% 1736   796.3KB/s   00:00  
service-account-key.pem               100% 1675   648.5KB/s   00:00  
service-account.pem                   100% 1399   635.6KB/s   00:00 


#ETCD
for i in master{01..03};do echo "+----------${i}----------+";scp ca.pem etcd*.pem ${i}:/opt/etcd/ssl;done
+----------master01----------+
ca.pem                                100% 1281    59.2KB/s   00:00  
etcd-key.pem                          100% 1679     1.2MB/s   00:00  
etcd.pem                              100% 1452     1.2MB/s   00:00  
+----------master02----------+
ca.pem                                100% 1281   327.3KB/s   00:00  
etcd-key.pem                          100% 1679   339.8KB/s   00:00  
etcd.pem                              100% 1452   101.0KB/s   00:00  
+----------master03----------+
ca.pem                                100% 1281   593.4KB/s   00:00  
etcd-key.pem                          100% 1679   749.0KB/s   00:00  
etcd.pem                              100% 1452   695.0KB/s   00:00 

//3node
for i in master{01,02} node01;do echo "+----------${i}----------+";scp ca.pem etcd*.pem ${i}:/opt/etcd/ssl;done
```

​    

# Etcd部署

整个kubernetes的数据存储池，calico数据存储

```BASH
[root@master01 /server/soft]# tar xf etcd-v3.5.0-linux-amd64.tar.gz 
[root@master01 /server/soft]#  cp etcd-v3.5.0-linux-amd64/etcd* /usr/local/bin
[root@master01 ~]# scp /usr/local/bin/etcd* master02:/usr/local/bin
[root@master01 ~]# scp /usr/local/bin/etcd* master03:/usr/local/bin


ETCD_IP=$(hostname -i)
ETCD_NAME=$(hostname -s)
ETCD_PATH="/opt/etcd/ssl/"

cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=ETCD Server
Documentation=https://github.com/coreos/etcd
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --name ${ETCD_NAME} \\
  --cert-file ${ETCD_PATH}etcd.pem \\
  --key-file ${ETCD_PATH}etcd-key.pem \\
  --peer-cert-file ${ETCD_PATH}etcd.pem \\
  --peer-key-file ${ETCD_PATH}etcd-key.pem \\
  --trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-trusted-ca-file ${ETCD_PATH}ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-client-urls https://${ETCD_IP}:2379 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --initial-cluster master01=https://172.16.0.30:2380,master02=https://172.16.0.31:2380,master03=https://172.16.0.32:2380 \\
  --initial-cluster-token etcd_cluster \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd/default.etcd 
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```

启动服务

三台节点这里注意！

```BASH
systemctl daemon-reload && systemctl start etcd && systemctl enable etcd

[root@master01 ~]#  for host in master{01..03} ;do ssh $host systemctl status etcd | grep Active ;done
   Active: active (running) since Tue 2021-07-13 21:24:06 CST; 20s ago
   Active: active (running) since Tue 2021-07-13 21:24:06 CST; 21s ago
   Active: active (running) since Tue 2021-07-13 21:24:08 CST; 19s ago

[root@master01 ~]#  ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/etcd.pem \
  --key /opt/etcd/ssl/etcd-key.pem \
  endpoint health \
  --write-out=table
+--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR |
+--------------------------+--------+-------------+-------+
| https://172.16.0.30:2379 |   true |  9.527365ms |       |
| https://172.16.0.31:2379 |   true |  9.672747ms |       |
| https://172.16.0.32:2379 |   true | 11.781275ms |       |
+--------------------------+--------+-------------+-------+

[root@master01 ~]# ETCDCTL_API=3 etcdctl member list \
   --endpoints=https://172.16.0.30:2379 \
   --cacert=/opt/etcd/ssl/ca.pem \
   --cert=/opt/etcd/ssl/etcd.pem \
   --key=/opt/etcd/ssl/etcd-key.pem \
     endpoint health \
   --write-out=table
+------------------+---------+----------+--------------------------+--------------------------+------------+
|        ID        | STATUS  |   NAME   |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+---------+----------+--------------------------+--------------------------+------------+
| 322eb3d030ebf559 | started | master01 | https://172.16.0.30:2380 | https://172.16.0.30:2379 |      false |
| 7015b46a66a16d1d | started | master03 | https://172.16.0.32:2380 | https://172.16.0.32:2379 |      false |
| d3fd314d63fa3eba | started | master02 | https://172.16.0.31:2380 | https://172.16.0.31:2379 |      false |
+------------------+---------+----------+--------------------------+--------------------------+------------+

```

较大集群使用负载均衡，使用参数 --etcd-servers=$LB:2379 

删除etcd中的数据

```BASH
#查看使用状态
ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/etcd.pem \
  --key /opt/etcd/ssl/etcd-key.pem \
 endpoint status \
  --write-out=table
   
   #查看警告信息alarm list
   ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/etcd.pem \
  --key /opt/etcd/ssl/etcd-key.pem \
  alarm list 
  
  
ETCDCTL_API=3 etcdctl \
  --endpoints https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \
  --cacert /opt/etcd/ssl/ca.pem \
  --cert /opt/etcd/ssl/etcd.pem \
  --key /opt/etcd/ssl/etcd-key.pem 
```





# Docker

虽然kubernetes 1.20以后不支持Docker 了但是还是可以搭建起来的。

```BASH
yum install -y yum-utils   device-mapper-persistent-data   lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo  #设置docker源
yum list docker-ce --showduplicates | sort -r #查看docker 版本

[root@master01 ~]# cd /server/soft/
[root@master01 /server/soft]# tar xf docker-20.10.7.tgz 
[root@master01 /server/soft]# for i in master{01..03} node{01,02};do scp docker/* $i:/usr/local/bin;done
```

systemd

```BASH
cat > /etc/systemd/system/docker.service << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/local/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s
[Install]
WantedBy=multi-user.target
EOF
```

share

```BASH
for i in master{02,03} node{01,02};do scp /etc/systemd/system/docker.service $i:/etc/systemd/system;done
```

启动

```BASH
systemctl daemon-reload && systemctl start docker && systemctl enable docker && systemctl status docker |grep Active
```

检查

```BASH
[root@master01 /server/soft]# sh ~/scp_file_all.sh /usr/local/bin/docker-compose /usr/local/bin
---------------------------File: master01 Send start ------------------------
docker-compose                                                       100%   12MB 107.6MB/s   00:00    
File	: /usr/local/bin/docker-compose Send Successfull
---------------------------File: /usr/local/bin/docker-compose  Send end ----

[root@master01 /server/soft]# docker --version && docker-compose --version
Docker version 20.10.7, build f0df350
docker-compose version 1.29.2, build 5becea4c

```

docker-compose

```sh
[root@master01 /server/soft]# chmod +x docker-compose-Linux-x86_64 
[root@master01 /server/soft]# for i in master{01..03} node01;do scp docker-compose-Linux-x86_64 $i:/usr/local/bin/docker-compose ;done
docker-compose-Linux-x86_64                                        100%   25MB  36.5MB/s   00:00    
docker-compose-Linux-x86_64                                        100%   25MB  26.8MB/s   00:00    
docker-compose-Linux-x86_64                                        100%   25MB  33.4MB/s   00:00    
docker-compose-Linux-x86_64                                        100%   25MB  31.9MB/s   00:00    

```



### daemon.json

```BASH
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

systemctl daemon-reload && systemctl restart docker

[root@master01 /server/soft]# for i in master{01..03} node{01,02};do ssh $i systemctl status docker|grep Active;done
   Active: active (running) since Tue 2021-07-13 22:00:30 CST; 2min 37s ago
   Active: active (running) since Tue 2021-07-13 22:00:31 CST; 2min 37s ago
   Active: active (running) since Tue 2021-07-13 22:00:31 CST; 2min 37s ago
   Active: active (running) since Tue 2021-07-13 22:00:31 CST; 2min 37s ago
   Active: active (running) since Tue 2021-07-13 22:00:30 CST; 2min 37s ago

```

# LB

## nginx

```BASH
[root@master01 /server/soft]# wget http://nginx.org/download/nginx-1.20.1.tar.gz

[root@master01 /server/soft]# tar xf nginx-1.20.1.tar.gz -C /opt

#创建nginx 用户（三台主控操作）
useradd -s /sbin/nologin -M nginx
#安装组件依赖包
yum install -y pcre pcre-devel openssl openssl-devel gcc-c++

传文件夹到另外两台主控
[root@master01 /opt/nginx-1.20.1]# scp -r /opt/nginx-1.20.1 master02:/opt/
[root@master01 /opt/nginx-1.20.1]# scp -r /opt/nginx-1.20.1 master03:/opt/

进入nginx 主目录
[root@master03 ~]# cd /opt/nginx-1.20.1/
[root@master03 /opt/nginx-1.20.1]# ./configure  \
--user=nginx \
--group=nginx \
--prefix=/usr/share/nginx \
--sbin-path=/usr/local/bin \
--conf-path=/etc/nginx/nginx.conf \
--error-log-path=/var/log/nginx/error.log \
--http-log-path=/var/log/nginx/access.log \
--pid-path=/var/run/nginx.pid \
--with-http_stub_status_module \
--with-http_ssl_module \
--with-stream 

Configuration summary
  + using system PCRE library
  + using system OpenSSL library
  + using system zlib library

  nginx path prefix: "/usr/share/nginx"
  nginx binary file: "/usr/local/bin"
  nginx modules path: "/usr/share/nginx/modules"
  nginx configuration prefix: "/etc/nginx"
  nginx configuration file: "/etc/nginx/nginx.conf"
  nginx pid file: "/var/run/nginx.pid"
  nginx error log file: "/var/log/nginx/error.log"
  nginx http access log file: "/var/log/nginx/access.log"
  nginx http client request body temporary files: "client_body_temp"
  nginx http proxy temporary files: "proxy_temp"
  nginx http fastcgi temporary files: "fastcgi_temp"
  nginx http uwsgi temporary files: "uwsgi_temp"
  nginx http scgi temporary files: "scgi_temp"

[root@master03 /opt/nginx-1.20.1]# make && make install
[root@master01 /opt/nginx-1.20.1]# nginx -v
nginx version: nginx/1.20.1

修改用户和添加索引文件
[root@master01 ~]# vim /etc/nginx/nginx.conf
user  nginx;
worker_processes  auto;

include /etc/nginx/conf.d/k8s.conf;
[root@master01 /opt/nginx-1.20.1]# mkdir -p /etc/nginx/conf.d

cat  /etc/nginx/conf.d/k8s.conf 
stream {
	log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';
    access_log  /var/log/nginx/k8s-access.log  main;

	upstream k8s-apiserver {
		server 172.16.0.30:6443;  
		server 172.16.0.31:6443;  
		server 172.16.0.32:6443;  
						   }
				    
	server {
		listen 16443; 
		proxy_pass k8s-apiserver;
		   }
}


[root@master01 /opt/nginx-1.20.1]# scp /etc/nginx/conf.d/k8s.conf master02:/etc/nginx/conf.d/

[root@master01 /opt/nginx-1.20.1]# scp /etc/nginx/conf.d/k8s.conf master03:/etc/nginx/conf.d/

```

测试语法

```BASH
[root@master01 /opt/nginx-1.20.1]# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
```

systemd

```BASH
cat >/etc/systemd/system/nginx.service<<EOF 
[Unit]
Description=Nginx Server
Documentation=http://nginx.org
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
ExecStartPre=/usr/local/bin/nginx -t
ExecStart=/usr/local/bin/nginx
PIDFile=/var/run/nginx.pid 
ExecReload=/usr/local/bin/nginx -s reload
ExecStop=/usr/local/bin/nginx  -s stop
PrivateTmp=true
[Install]
WantedBy=multi-user.target
EOF

[root@master01 /opt/nginx-1.20.1]# systemctl daemon-reload && systemctl start nginx && systemctl enable nginx

[root@master01 /opt/nginx-1.20.1]# for i in master{02,03};do scp /etc/systemd/system/nginx.service $i:/etc/systemd/system/;done

[root@master01 /opt/nginx-1.20.1]# for i in master{01..03};do ssh $i systemctl status nginx |grep Active;done
   Active: active (running) since Mon 2021-10-18 14:38:09 CST; 3min 11s ago
   Active: active (running) since Mon 2021-10-18 14:40:05 CST; 1min 15s ago
   Active: active (running) since Mon 2021-10-18 14:40:08 CST; 1min 12s ago

```

## keepalive

```BASH
yum install -y keepalived
```

配置

```BASH
[root@master01 /opt/nginx-1.20.1]# cat > /etc/keepalived/keepalived.conf <<EOF
! Configuration File for keepalived

global_defs {
   router_id NGINX_MASTER
}

#check_nginx
vrrp_script check_nginx {
		    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.37/24
    }

#track_script
track_script {
    check_nginx
	} 
}
EOF
```

脑裂脚本

```BASH
[root@master01 /opt/nginx-1.20.1]# cat /etc/keepalived/check_nginx.sh 
#!/bin/bash
count=$(ps -ef|grep -c [n]ginx)
if [ "$count" -le 2 ];then
    systemctl stop keepalived
fi

chmod +x /etc/keepalived/check_nginx.sh
```

传文件

```BASH
[root@master01 /opt/nginx-1.20.1]# for i in master{02,03};do scp -r /etc/keepalived/ $i:/etc/ ;done
keepalived.conf                       100%  476   392.7KB/s   00:00    
check_nginx.sh                        100%  105   112.6KB/s   00:00    
keepalived.conf                       100%  476   442.2KB/s   00:00    
check_nginx.sh                        100%  105   109.1KB/s   00:00    

```

修改master02

```BASH
[root@master02 /opt/nginx-1.20.1]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived

global_defs {
   router_id NGINX_BACKUP01
}

#check_nginx
vrrp_script check_nginx {
		    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state BACKUP01
    interface eth0
    virtual_router_id 51
    priority 125
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.37/24
    }

#track_script
    track_script {
		        check_nginx
				} 

}

```



修改master03

```BASH
[root@master03 /opt/nginx-1.20.1]# cat /etc/keepalived/keepalived.conf 
! Configuration File for keepalived

global_defs {
   router_id NGINX_BACKUP02
}

#check_nginx
vrrp_script check_nginx {
		    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state BACKUP02
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.0.37/24
    }

#track_script
    track_script {
		        check_nginx
				} 

}

```

启动

```BASH
[root@master01 ~]# systemctl daemon-reload && systemctl start keepalived && systemctl enable keepalived

[root@master01 /opt/nginx-1.20.1]# for i in master{01..03};do ssh $i systemctl status nginx keepalived|grep Active;done
   Active: active (running) since Tue 2021-07-13 23:00:28 CST; 9min ago
   Active: active (running) since Tue 2021-07-13 23:08:46 CST; 55s ago
   Active: active (running) since Tue 2021-07-13 23:00:31 CST; 9min ago
   Active: active (running) since Tue 2021-07-13 23:08:43 CST; 59s ago
   Active: active (running) since Tue 2021-07-13 23:00:34 CST; 9min ago
   Active: active (running) since Tue 2021-07-13 23:08:40 CST; 1min 1s ago

```

VIP

```BASH
[root@master01 /opt/nginx-1.20.1]# ip a s eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:55:fb:ab brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.30/24 brd 172.16.0.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
    inet 172.16.0.37/24 scope global secondary eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::6167:ad52:bc17:7529/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever

```

LB就搭建好了



# kubernetes

## 文件准备

```BASH
[root@master01 /server/soft]# tar xf kubernetes-server-linux-amd64_1.21.2.tar.gz 
[root@master01 /server/soft]# cd kubernetes/server/bin/
[root@master01 /server/soft/kubernetes/server/bin]# for i in master{01..03} node{01,02};do scp kubelet kube-proxy kubectl $i:/usr/local/bin;done
kubelet                              100%  113MB 141.7MB/s   00:00    
kube-proxy                           100%   41MB 135.5MB/s   00:00    
kubectl                              100%   44MB 148.9MB/s   00:00    
kubelet                              100%  113MB 128.0MB/s   00:00    
kube-proxy                           100%   41MB 134.1MB/s   00:00    
kubectl                              100%   44MB 138.8MB/s   00:00    
kubelet                              100%  113MB 127.5MB/s   00:00    
kube-proxy                           100%   41MB 129.3MB/s   00:00    
kubectl                              100%   44MB 126.0MB/s   00:00    
kubelet                              100%  113MB 133.6MB/s   00:00    
kube-proxy                           100%   41MB 118.0MB/s   00:00    
kubectl                              100%   44MB 123.5MB/s   00:00    
kubelet                              100%  113MB 132.6MB/s   00:00    
kube-proxy                           100%   41MB 117.6MB/s   00:00    
kubectl                              100%   44MB 120.8MB/s   00:00    

[root@master01 /server/soft/kubernetes/server/bin]# for i in master{01..03};do scp kubeadm kube-apiserver kube-controller-manager kube-scheduler $i:/usr/local/bin;done
kubeadm                                    100%   43MB  72.1MB/s   00:00    
kube-apiserver                             100%  116MB 116.4MB/s   00:01    
kube-controller-manager                    100%  111MB 110.9MB/s   00:01    
kube-scheduler                             100%   45MB 102.6MB/s   00:00    
kubeadm                                    100%   43MB 115.5MB/s   00:00    
kube-apiserver                             100%  116MB 118.4MB/s   00:00    
kube-controller-manager                    100%  111MB 117.0MB/s   00:00    
kube-scheduler                             100%   45MB 109.0MB/s   00:00    
kubeadm                                    100%   43MB 105.4MB/s   00:00    
kube-apiserver                             100%  116MB 123.4MB/s   00:00    
kube-controller-manager                    100%  111MB 128.2MB/s   00:00    
kube-scheduler                             100%   45MB 111.7MB/s   00:00    

```

## kubeconfig

kubeconfig文件包含集群参数 (CA证书、API Server地址)，==客户端==参数(上面生成的证书和私钥)，集群context信息(集群名称、用户名) Kubenetes 组件通过启动时指定不同的kubeconfig 文件可以切换到不同的`集群`

| 文件名                  | 命令                    | 说明                                                       |
| ----------------------- | ----------------------- | ---------------------------------------------------------- |
| admin.conf              | kubectl                 | 配置集群的管理员                                           |
| kubelet.conf            | kubelet                 | 集群中的每个节点都需要一份                                 |
| controller-manager.conf | kube-controller-manager | 必需添加到 `manifests/kube-controller-manager.yaml` 清单中 |
| scheduler.conf          | kube-scheduler          | 必需添加到 `manifests/kube-scheduler.yaml` 清单中          |

生成kubeconfig配置文件固定格式

```BASH

kubectl config set-cluster default-cluster \
  --certificate-authority=ca.pem \
  --server=https://<host ip>:6443 \
  --embed-certs=true \
  --kubeconfig=<service>.kubeconfig
  
kubectl config set-credentials  <credential-name> \
  --client-key=<service>-key.pem \
  --client-certificate=<service>.pem \
  --embed-certs=true \
  --kubeconfig=<service>.kubeconfig
   
kubectl config set-context default-system \
  --cluster=default-cluster  \
  --user=<credential-name> \
  --kubeconfig=<service>.kubeconfig
  
kubectl config use-context default-system  --kubeconfig=<service>.kubeconfig
```

### kubelet

```BASH
LB_VIP="https://172.16.0.37:16443"

for i in master{01..03} node{01,02}; do

  kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${LB_VIP} \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-credentials system:node:${i} \
    --client-certificate=${i}.pem \
    --client-key=${i}-key.pem \
    --embed-certs=true \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${i} \
    --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig

  kubectl config use-context default --kubeconfig=/opt/kubernetes/cfg/${i}.kubeconfig
done
```

这里采用艰难方式手动生成kubelet.kubeconfig 

```mermaid
graph LR
A[认证] --> B[pod]
A[认证] --> C[Kubernetes组件]
B --> D[server account]
D --> D1[service-account-tonken]
D1 --> H
C --> E[kubelet,kube-proxy]
C --> F[kubelet--TSL bootstrap]
F --> G[证书-kubeconfig]
E --> |手动签发| G
G --> H[APIserver]


```



### kube-proxy

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"
LB_VIP="https://172.16.0.37:16443"

kubectl config set-cluster kubernetes  \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${LB_VIP} \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-proxy \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
```

### kube-controller-manager

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
```

### kube-scheduler

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"

 kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

### admin

```BASH
KUBE_CONFIG="/opt/kubernetes/cfg/admin.kubeconfig"

kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=admin \
    --kubeconfig=${KUBE_CONFIG}

  kubectl config use-context default --kubeconfig=${KUBE_CONFIG}
```

这里使用LB_VIP目的是为了在管理节点主机能够访问

管理节点添加admin访问权限

```BASH
[root@master01 ~/tls]# cp /opt/kubernetes/cfg/admin.kubeconfig ~/.kube/config
[root@master01 ~/tls]# scp ~/.kube/config master02:~/.kube/
config                                                               100% 6150     2.4MB/s   00:00    
[root@master01 ~/tls]# scp ~/.kube/config master03:~/.kube/
config                                                               100% 6150     3.2MB/s   00:00    
```

分发kubeconfig

```BASH
[root@master01 ~/tls]# for i in master{02,03} node{01,02};do scp /opt/kubernetes/cfg/{kube-proxy,$i}.kubeconfig $i:/opt/kubernetes/cfg ;done
kube-proxy.kubeconfig                                                  100% 6182     4.1MB/s   00:00    
master02.kubeconfig                                                    100% 6412     4.8MB/s   00:00    
kube-proxy.kubeconfig                                                  100% 6182   254.1KB/s   00:00    
master03.kubeconfig                                                    100% 6412     2.6MB/s   00:00    
kube-proxy.kubeconfig                                                  100% 6182   599.5KB/s   00:00    
node01.kubeconfig                                                      100% 6408     1.9MB/s   00:00    
kube-proxy.kubeconfig                                                  100% 6182     1.5MB/s   00:00    
node02.kubeconfig                                                      100% 6404     1.7MB/s   00:00    

[root@master01 ~/tls]# for i in master{02,03};do scp /opt/kubernetes/cfg/{kube-controller-manager,kube-scheduler}.kubeconfig $i:/opt/kubernetes/cfg ;done
kube-controller-manager.kubeconfig                                   100% 6261     4.3MB/s   00:00    
kube-scheduler.kubeconfig                                            100% 6203     6.3MB/s   00:00    
kube-controller-manager.kubeconfig                                   100% 6261     2.8MB/s   00:00    
kube-scheduler.kubeconfig                                            100% 6203     4.4MB/s   00:00    
```

### 生成密钥文件

```BASH
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

cat > /opt/kubernetes/ssl/encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF

[root@master01 ~/tls]# for i in master{02,03};do scp /opt/kubernetes/ssl/encryption-config.yaml $i:/opt/kubernetes/ssl ;done
encryption-config.yaml                                               100%  240   101.8KB/s   00:00    
encryption-config.yaml                                               100%  240   172.1KB/s   00:00    
```

## systemd

### apiserver

```BASH
KUBE_APISERVER=$(hostname -i)


cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --log-dir=/opt/kubernetes/logs \\
  --advertise-address=${KUBE_APISERVER} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/opt/etcd/ssl/ca.pem \\
  --etcd-certfile=/opt/etcd/ssl/etcd.pem \\
  --etcd-keyfile=/opt/etcd/ssl/etcd-key.pem \\
  --etcd-servers=https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/opt/kubernetes/ssl/encryption-config.yaml \\
  --kubelet-certificate-authority=/opt/kubernetes/ssl/ca.pem \\
  --kubelet-client-certificate=/opt/kubernetes/ssl/kube-apiserver.pem \\
  --kubelet-client-key=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/opt/kubernetes/ssl/service-account.pem \\
  --service-account-signing-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
  --service-account-issuer=api \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/opt/kubernetes/ssl/kube-apiserver.pem \\
  --tls-private-key-file=/opt/kubernetes/ssl/kube-apiserver-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

```

启动

```BASH
systemctl daemon-reload && systemctl start kube-apiserver && systemctl enable kube-apiserver
systemctl daemon-reload && systemctl restart kube-apiserver 
```

所有控制节点都做这个操作

### kube-controller-manager

```BASH
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --log-dir=/opt/kubernetes/logs \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.244.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\
  --kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/opt/kubernetes/ssl/service-account-key.pem \\
  --service-cluster-ip-range=10.0.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动

```BASH
systemctl daemon-reload && systemctl start kube-controller-manager && systemctl enable kube-controller-manager
```

主控节点都部署

### kube-scheduler

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --log-dir=/opt/kubernetes/logs \\
  --config=/opt/kubernetes/cfg/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动

```BASH
systemctl daemon-reload && systemctl start kube-scheduler && systemctl enable kube-scheduler
```

### 服务脚本检查

```BASH
echo "Nginx keepalived_Check......"
#nginx,keepalived检查
echo "+-------------------------------------------------------+";
for host in master{01..03};do for i in nginx keepalived;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";


echo "Kube-apiserver_Check......"
#apiserver 检查
echo "+-------------------------------------------------------+";
for host in master{01..03};do for i in kube-apiserver kube-controller-manager kube-scheduler;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";

echo "Etcd_Check......"
#etcd检查
echo "+-------------------------------------------------------+";
for host in master{01..03};do echo -e "    $host  etcd is  |  \c" && ssh $host systemctl status etcd|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";

echo "docker_Check......"
echo "+-------------------------------------------------------+";
#docker检查
for host in master{01..03} node{01,02};do echo -e "    $host  docker is  |  \c" && ssh $host systemctl status docker|grep Active |awk -F"[()]" '{print $2}' ;done
echo "+-------------------------------------------------------+";


echo "Kube-proxy kubelet_Check......"
#kubelet,proxy检查
echo "+-------------------------------------------------------+";
for host in master{01..03} node{01,02};do for i in kube-proxy kubelet;
do echo -e "    $host  $i is : \c" && ssh $host systemctl status $i|grep Active |awk -F"[()]" '{print $2}' ;done;done
echo "+-------------------------------------------------------+";                                                                            
```

```BASH
[root@master01 ~]# sh check_service.sh 
Nginx keepalived_Check......
+-------------------------------------------------------+
    master01  nginx is : running
    master01  keepalived is : running
    master02  nginx is : running
    master02  keepalived is : running
    master03  nginx is : running
    master03  keepalived is : running
+-------------------------------------------------------+
Kuoe-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : running
    master01  kube-controller-manager is : running
    master01  kube-scheduler is : running
    master02  kube-apiserver is : running
    master02  kube-controller-manager is : running
    master02  kube-scheduler is : running
    master03  kube-apiserver is : running
    master03  kube-controller-manager is : running
    master03  kube-scheduler is : running
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
Docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : Unit kube-proxy.service could not be found.
    master01  kubelet is : Unit kubelet.service could not be found.
    master02  kube-proxy is : Unit kube-proxy.service could not be found.
    master02  kubelet is : Unit kubelet.service could not be found.
    master03  kube-proxy is : Unit kube-proxy.service could not be found.
    master03  kubelet is : Unit kubelet.service could not be found.
    node01  kube-proxy is : Unit kube-proxy.service could not be found.
    node01  kubelet is : Unit kubelet.service could not be found.
    node02  kube-proxy is : Unit kube-proxy.service could not be found.
    node02  kubelet is : Unit kubelet.service could not be found.
+-------------------------------------------------------+

```

查看服务器状态

```BASH
[root@master01 ~]# kubectl get cs  （三台控制端都准备就绪）
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-1               Healthy   {"health":"true","reason":""}   
etcd-2               Healthy   {"health":"true","reason":""}  
```



### RBAC

```BASH
任意一台master操作
#创建system:kube-apiserver-to-kubelet ClusterRole有权限访问Kubelet API，并执行与管理相关的pod最常见的任务

cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF

----------------------------------------------------
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created

#The Kubernetes API Server authenticates to the Kubelet as the kubernetes user using the client certificate as defined by the --kubelet-client-certificate flag.
#Bind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:

cat <<EOF | kubectl apply --kubeconfig /opt/kubernetes/cfg/admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF

----------------------------------
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created

```

检查一下部署是否正常

```BASH
[root@master01 ~]# curl --cacert /opt/kubernetes/ssl/ca.pem -i https://127.0.0.1:6443/version
HTTP/1.1 200 OK
Cache-Control: no-cache, private
Content-Type: application/json
X-Kubernetes-Pf-Flowschema-Uid: b5743420-11c7-4907-945d-7492f3674e5d
X-Kubernetes-Pf-Prioritylevel-Uid: 9c74bbff-4b29-4360-80b6-11e09cec56fb
Date: Thu, 15 Jul 2021 02:14:27 GMT
Content-Length: 263

{
  "major": "1",
  "minor": "21",
  "gitVersion": "v1.21.2",
  "gitCommit": "092fbfbf53427de67cac1e9fa54aaa09a28371d7",
  "gitTreeState": "clean",
  "buildDate": "2021-06-16T12:53:14Z",
  "goVersion": "go1.16.5",
  "compiler": "gc",
  "platform": "linux/amd64"
}

```

上面只是单台正常，apiserver集群的呢？

```BASH
[root@master01 ~]# curl --cacert /opt/kubernetes/ssl/ca.pem  -i  https://172.16.0.37:16443/version
HTTP/1.1 200 OK
Cache-Control: no-cache, private
Content-Type: application/json
X-Kubernetes-Pf-Flowschema-Uid: b5743420-11c7-4907-945d-7492f3674e5d
X-Kubernetes-Pf-Prioritylevel-Uid: 9c74bbff-4b29-4360-80b6-11e09cec56fb
Date: Thu, 15 Jul 2021 02:42:31 GMT
Content-Length: 263

{
  "major": "1",
  "minor": "21",
  "gitVersion": "v1.21.2",
  "gitCommit": "092fbfbf53427de67cac1e9fa54aaa09a28371d7",
  "gitTreeState": "clean",
  "buildDate": "2021-06-16T12:53:14Z",
  "goVersion": "go1.16.5",
  "compiler": "gc",
  "platform": "linux/amd64"
}
```



## Work 部署

### kubelet

```BASH
echo 'mkdir -p /run/systemd/resolve &&  ln -s /etc/resolv.conf /run/systemd/resolve/ '  >>/etc/rc.d/rc.local	
chmod +x /etc/rc.d/rc.local 

cat <<EOF | sudo tee /opt/kubernetes/cfg/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/opt/kubernetes/ssl/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.0.0.2"
podCIDR: "10.0.0.0/24"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/opt/kubernetes/ssl/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/opt/kubernetes/ssl/${HOSTNAME}-key.pem"
EOF

```

systemd

```BASH
KUBELET_IP=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --log-dir=/opt/kubernetes/logs \\
  --config=/opt/kubernetes/cfg/kubelet-config.yaml \\
  --container-runtime=docker \\
  --container-runtime-endpoint=unix:///var/run/dockershim.sock   \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/opt/kubernetes/cfg/${KUBELET_IP}.kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --cgroup-driver=systemd \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


unix:///var/run/dockershim.sock    docker
unix:///var/run/containerd/containerd.sock   containerd
```

启动

```BASH
systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet

systemctl daemon-reload && systemctl restart kubelet

```

### kube-proxy

```BASH
cat <<EOF | sudo tee /opt/kubernetes/cfg/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/opt/kubernetes/cfg/kube-proxy.kubeconfig"
mode: "iptables"
clusterCIDR: "10.244.0.0/16"
EOF
```



systemd

```BASH
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --log-dir=/opt/kubernetes/logs \\
  --config=/opt/kubernetes/cfg/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

启动

```BASH
systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy
```

### 检查

```BASH
[root@master01 ~]# sh check_service.sh 
Nginx keepalived_Check......
+-------------------------------------------------------+
    master01  nginx is : running
    master01  keepalived is : running
    master02  nginx is : running
    master02  keepalived is : running
    master03  nginx is : running
    master03  keepalived is : running
+-------------------------------------------------------+
Kuoe-apiserver_Check......
+-------------------------------------------------------+
    master01  kube-apiserver is : running
    master01  kube-controller-manager is : running
    master01  kube-scheduler is : running
    master02  kube-apiserver is : running
    master02  kube-controller-manager is : running
    master02  kube-scheduler is : running
    master03  kube-apiserver is : running
    master03  kube-controller-manager is : running
    master03  kube-scheduler is : running
+-------------------------------------------------------+
Etcd_Check......
+-------------------------------------------------------+
    master01  etcd is  |  running
    master02  etcd is  |  running
    master03  etcd is  |  running
+-------------------------------------------------------+
Docker_Check......
+-------------------------------------------------------+
    master01  docker is  |  running
    master02  docker is  |  running
    master03  docker is  |  running
    node01  docker is  |  running
    node02  docker is  |  running
+-------------------------------------------------------+
Kube-proxy kubelet_Check......
+-------------------------------------------------------+
    master01  kube-proxy is : running
    master01  kubelet is : running
    master02  kube-proxy is : running
    master02  kubelet is : running
    master03  kube-proxy is : running
    master03  kubelet is : running
    node01  kube-proxy is : running
    node01  kubelet is : running
    node02  kube-proxy is : running
    node02  kubelet is : running
+-------------------------------------------------------+

```

```BASH
[root@master01 ~]# kubectl get node
NAME       STATUS     ROLES    AGE     VERSION
master01   NotReady   <none>   6h23m   v1.21.2
master02   NotReady   <none>   6h23m   v1.21.2
master03   NotReady   <none>   6h23m   v1.21.2
node01     NotReady   <none>   6h23m   v1.21.2
node02     NotReady   <none>   6h23m   v1.21.2

```

组件版本查看

```BASH
[root@master01 ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.21.2
k8s.gcr.io/kube-controller-manager:v1.21.2
k8s.gcr.io/kube-scheduler:v1.21.2
k8s.gcr.io/kube-proxy:v1.21.2
k8s.gcr.io/pause:3.4.1
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns/coredns:v1.8.0
```

### pause源码构建

第三方的不可信

```BASH
[root@master01 ~]# cd /server/soft/
[root@master01 /server/soft]# mkdir pause
[root@master01 /server/soft]# cd pause/
[root@master01 /server/soft/pause]# git clone https://github.com/kubernetes/kubernetes
Cloning into 'kubernetes'...
remote: Enumerating objects: 1252747, done.
remote: Counting objects: 100% (242/242), done.
remote: Compressing objects: 100% (171/171), done.
remote: Total 1252747 (delta 92), reused 110 (delta 67), pack-reused 1252505
Receiving objects: 100% (1252747/1252747), 766.40 MiB | 3.20 MiB/s, done.
Resolving deltas: 100% (902135/902135), done.
Checking out files: 100% (23194/23194), done.
[root@master01 /server/soft/pause]# cd kubernetes/build/pause/
[root@master01 /server/soft/pause/kubernetes/build/pause]# mkdir bin
[root@master01 /server/soft/pause/kubernetes/build/pause]# cd linux/
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# make pause
cc     pause.c   -o pause
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# gcc -Os -Wall -static -o pause pause.c/usr/bin/ld: cannot find -lc
collect2: error: ld returned 1 exit status
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# yum install glibc-static -y
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# gcc -Os -Wall -static -o pause pause.c
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# ls
orphan.c  pause  pause.c
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]#  file pause
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]#  nm pause
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# strip pause
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# file pause
pause: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.32, BuildID[sha1]=4522396d14e766d204882c9c2646ab4d1bbbefe2, stripped
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# nm pause
nm: pause: no symbols
[root@master01 /server/soft/pause/kubernetes/build/pause/linux]# cp pause ../bin/pause-linux-amd64
```

strip
通过上面的对比，可以看出strip后，pause文件由875K瘦身到801K。strip执行前后，不改变程序的执行能力。在开发过程中，strip用于产品的发布，调试均用未strip的程序。

file
通过file命令可以看到pause的strip状态

nm
通过nm命令，可以看到strip后的pause文件没有符号信息

```BASH
[root@master01 /server/soft/pause/kubernetes/build/pause]# cat Dockerfile
# Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG BASE
FROM ${BASE}
ARG ARCH
ADD bin/pause-linux-${ARCH} /pause
USER 65535:65535
ENTRYPOINT ["/pause"]

docker build  --build-arg BASE=scratch --build-arg ARCH=amd64 -t pause:3.4.1 .   

[root@master01 /server/soft/pause/kubernetes/build/pause]# docker build  --build-arg BASE=scratch --build-arg ARCH=adm64 -t pause:3.4.1 .   
Sending build context to Docker daemon  1.669MB
Step 1/6 : ARG BASE
Step 2/6 : FROM ${BASE}
 ---> 
Step 3/6 : ARG ARCH
 ---> Running in 952ca7ba15e3
Removing intermediate container 952ca7ba15e3
 ---> 5c124b53731a
Step 4/6 : ADD bin/pause-linux-${ARCH} /pause
 ---> 9c9f7e46fd56
Step 5/6 : USER 65535:65535
 ---> Running in 7a40b38c2f8e
Removing intermediate container 7a40b38c2f8e
 ---> 55d42d4d8b20
Step 6/6 : ENTRYPOINT ["/pause"]
 ---> Running in de2b3cd9c268
Removing intermediate container de2b3cd9c268
 ---> 0f5809a2bac7
Successfully built 0f5809a2bac7
Successfully tagged pause:3.4.1

```

修改成默认名称的镜像

```BASH
[root@master01 /server/soft/pause/kubernetes/build/pause]# docker images
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
pause        3.4.1     0f5809a2bac7   21 seconds ago   819kB
[root@master01 /server/soft/pause/kubernetes/build/pause]# docker tag 0f5809a2bac7 k8s.gcr.io/pause:3.4.1
[root@master01 /server/soft/pause/kubernetes/build/pause]# docker images
REPOSITORY         TAG       IMAGE ID       CREATED              SIZE
k8s.gcr.io/pause   3.4.1     0f5809a2bac7   About a minute ago   819kB
pause              3.4.1     0f5809a2bac7   About a minute ago   819kB

```

测试

```BASH
[root@master01 ~]# docker run -itd --name pause k8s.gcr.io/pause:3.4.1
d73c9969889bb5496af76b50373a2bc3201b4397246baaa7f5a3554216949ab7
[root@master01 ~]# docker run -itd --name busybox --net=container:pause --pid=container:pause --ipc=container:pause busybox:latest
这里IPC会出错
[root@master01 ~]# docker run -itd --name busybox2 --net=container:pause --pid=container:pause  busybox:latest
b9fc4eb7168d8cfeffe3eb074646cced795435a9ae331f32e011cb31008624a8

[root@master01 ~]# docker ps
CONTAINER ID   IMAGE                    COMMAND    CREATED              STATUS              PORTS     NAMES
b9fc4eb7168d   busybox:latest           "sh"       29 seconds ago       Up 28 seconds                 busybox2
d73c9969889b   k8s.gcr.io/pause:3.4.1   "/pause"   About a minute ago   Up About a minute             pause

[root@master01 ~]# docker exec -it b9fc4eb7168d /bin/sh
/ # ps aux
PID   USER     TIME  COMMAND
    1 65535     0:00 /pause
   10 root      0:00 sh
   16 root      0:00 /bin/sh
   24 root      0:00 ps aux
/ # 

```

#### pause打包

```BASH
[root@master01 ~]# docker save -o k8s.gcr.io_pause.tar k8s.gcr.io/pause
[root@master01 ~]# ls
anaconda-ks.cfg   check_ssh.sh          push_ssh_key.sh  tls      vimrc
check_service.sh  k8s.gcr.io_pause.tar  scp_file_all.sh  uuid.sh

[root@master01 ~]# docker rm -f `docker ps -a -q`    #删除运行的docker
[root@master01 ~]# docker image rm -f `docker images -a -q  `   #删除images

[root@master01 ~]# for i in master{01..03} node{01,02};do scp k8s.gcr.io_pause.tar $i:~ ;done
k8s.gcr.io_pause.tar                                                 100%  812KB  31.4MB/s   00:00    
k8s.gcr.io_pause.tar                                                 100%  812KB  48.2MB/s   00:00    
k8s.gcr.io_pause.tar                                                 100%  812KB  94.2MB/s   00:00    
k8s.gcr.io_pause.tar                                                 100%  812KB  74.1MB/s   00:00   
k8s.gcr.io_pause.tar                                                 100%  812KB  37.4MB/s   00:00    

```

#### pause 恢复

```BASH
[root@master01 ~]#  docker load < k8s.gcr.io_pause.tar 
```

# calico网络组件

## 安装cni(非必须)

```BASH
[root@master01 /server/soft]# tar  xf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin/

#创建bridge网络配置文件
cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.4.0",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "10.244.0.0/16"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF
#创建loopback网络配置文件
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
EOF

for i in master{02,03} node{01,02};do echo "+----------$i----------+";scp -r /opt/cni/ $i:/opt/;done
for i in master{02,03} node{01,02};do echo "+----------$i----------+";scp -r /etc/cni/ $i:/etc/;done
```



参考：https://docs.projectcalico.org/getting-started/kubernetes/hardway/

https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-etcd-datastore

```BASH
[root@master01 ~]# mkdir /opt/yaml && cd /opt/yaml/

[root@master01 /opt/yaml]# curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml

In the ConfigMap named, calico-config, set the value of etcd_endpoints to the IP address and port of your etcd server.
```

 cat <file> | base64 -w 0

```BASH
[root@master01 /opt/yaml]# cat /opt/etcd/ssl/ca.pem |base64 -w 0 >etcd-ca
[root@master01 /opt/yaml]# cat /opt/etcd/ssl/etcd.pem |base64 -w 0 >etcd-cert
[root@master01 /opt/yaml]# cat /opt/etcd/ssl/etcd-key.pem |base64 -w 0 >etcd-key
```

### 编辑calico.yaml文件

```BASH
第一处要修改的
# Source: calico/templates/calico-etcd-secrets.yaml
# The following contains k8s Secrets for use with a TLS enabled etcd cluster.
# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: calico-etcd-secrets
  namespace: kube-system
data:
  # Populate the following with etcd TLS configuration if desired, but leave blank if
  # not using TLS for etcd.
  # The keys below should be uncommented and the values populated with the base64
  # encoded contents of each file that would be associated with the TLS data.
  # Example command for encoding a file contents: cat <file> | base64 -w 0
  etcd-key: 
  etcd-cert: 
  etcd-ca: 
  填写刚才加密的三个文件
  
  第二处要修改的
  kind: ConfigMap
 27 apiVersion: v1
 28 metadata:
 29   name: calico-config
 30   namespace: kube-system
 31 data:
 32   # Configure this with the location of your etcd cluster.
 33   etcd_endpoints: "https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379"
 34   # If you're using TLS enabled etcd uncomment the following.
 35   # You must also populate the Secret below with these files.
 36   etcd_ca: "/calico-secrets/etcd-ca"
 37   etcd_cert: "/calico-secrets/etcd-cert"
 38   etcd_key: "/calico-secrets/etcd-key"

第三处  先不修改看看能否成功，因为我规划了不同网段的pod_cidr
360             # - name: CALICO_IPV4POOL_CIDR
361             #   value: "192.168.0.0/16"  
360             - name: CALICO_IPV4POOL_CIDR
361               value: "10.244.0.0/16" 

service_cluster_ip_range=10.0.0.0/24
cluster_cidr=10.244.0.0/16
pod-cidr=10.244.0.0/16
```

创建

```BASH
kubectl apply -f calico.yaml
[root@master01 /opt/yaml]# kubectl create -f calico.yaml 
secret/calico-etcd-secrets created
configmap/calico-config created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-kube-controllers created
```

提示修改

```BASH
apiVersion: policy/v1                                                                                                                                  
kind: PodDisruptionBudget
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: calico-kube-controllers

```



等一会镜像下载完成

```BASH
[root@master01 ~]# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
master01   Ready    <none>   8h    v1.21.2
master02   Ready    <none>   8h    v1.21.2
master03   Ready    <none>   8h    v1.21.2
node01     Ready    <none>   8h    v1.21.2
node02     Ready    <none>   8h    v1.21.2

[root@master01 ~]# kubectl get pod -n kube-system 
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-5fc6448d7c-npmr6   1/1     Running   0          24m
calico-node-8d9x2                          1/1     Running   0          24m
calico-node-p4p76                          1/1     Running   0          24m
calico-node-rlr8c                          1/1     Running   0          24m
calico-node-s55p8                          1/1     Running   0          24m
calico-node-tltbg                          1/1     Running   0          24m

```



### 问题处理

安装calicoctl

https://docs.projectcalico.org/archive/v3.19/getting-started/clis/calicoctl/install

```BASH
curl -o calicoctl -O -L  "https://github.com/projectcalico/calicoctl/releases/download/v3.19.1/calicoctl" 
chmod +x calicoctl
mv calicoctl /usr/local/bin
```

自定义资源

https://docs.projectcalico.org/archive/v3.19/getting-started/kubernetes/hardway/the-calico-datastore

```BASH
wget https://docs.projectcalico.org/archive/v3.19/manifests/crds.yaml
kubectl apply -f crds.yaml

kubectl apply -f crds.yaml

[root@master01 /opt/yaml]# calicoctl create -f pool1.yaml
Successfully created 1 'IPPool' resource(s)
[root@master01 /opt/yaml]# calicoctl create -f pool2.yaml
Successfully created 1 'IPPool' resource(s)

[root@master01 /opt/yaml]# calicoctl get ippool -o wide
NAME    CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR   
pool1   10.244.0.0/16    true   Never      Never       false      all()      
pool2   192.168.0.0/18   true   Never      Never       true       all()      

```



修改calico IP地址池

https://docs.projectcalico.org/networking/migrate-pools

```BASH
wget https://github.com/projectcalico/calicoctl/releases/download/v3.14.0/calicoctl
chmod +x calicoctl
sudo mv calicoctl /usr/local/bin/


export CALICO_DATASTORE_TYPE=kubernetes
export CALICO_KUBECONFIG=~/.kube/config
export ETCD_ENDPOINTS=https://172.16.0.30:2379,https://172.16.0.31:2379,https://172.16.0.32:2379


在大多数系统上，kubeconfig 位于~/.kube/config. 您可能希望将这些export行添加到您的行中，~/.bashrc以便下次登录时它们会保留下来。

[root@master01 /opt/yaml]# calicoctl get ippools
NAME   CIDR   SELECTOR  

cat > pool1.yaml <<EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: pool1
spec:
  cidr:  10.244.0.0/16
  ipipMode: Never
  natOutgoing: true
  disabled: false
  nodeSelector: all()
EOF


cat > pool2.yaml <<EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: pool2
spec:
  cidr: 192.168.0.0/18
  ipipMode: Never
  natOutgoing: true
  disabled: true
  nodeSelector: all()
EOF

在第二个池中，我们设置disabled为true，这意味着 Calico 不会创建具有池中地址的新 Pod，但仍会将具有这些地址的 Pod 识别为 Calico 网络的一部分。稍后，在 测试网络实验室中，我们将启用此池并演示如何控制您的 Pod 从哪些池分配地址。

这nodeSelector是一个标签选择器，用于确定哪些节点使用池。它们都设置为all()意味着所有节点都可以使用池。

[root@master01 /opt/yaml]# calicoctl get ippool -o wide
NAME    CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR   
pool1   10.244.0.0/16    true   Never      Never       false      all()      
pool2   192.168.0.0/18   true   Never      Never       true       all()      

删除calico.yaml
检查所有节点上的网络
ip a s
删除tunl0
modprobe -r ipip
[root@master01 /opt/yaml]# rm -rf /etc/cni/net.d/*

reboot

[root@master01 ~]# kubectl get node
NAME       STATUS     ROLES    AGE   VERSION
master01   NotReady   <none>   20h   v1.21.2
master02   NotReady   <none>   20h   v1.21.2
master03   NotReady   <none>   20h   v1.21.2
node01     NotReady   <none>   20h   v1.21.2
node02     NotReady   <none>   20h   v1.21.2


```



# CoreDNS

```BASH
修改DNS
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.0.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP          
[root@master01 /opt/yaml]# kubectl create -f coredns-1.8.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created

[root@master01 /opt/yaml]# kubectl apply -f coredns-1.8.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created

[root@master01 /opt/yaml]# kubectl get pod -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES
calico-kube-controllers-5fc6448d7c-fs8rb   1/1     Running   1          6h27m   172.16.0.30     master01   <none>           <none>
calico-node-2wvnh                          1/1     Running   1          6h27m   172.16.0.35     node01     <none>           <none>
calico-node-9dtwt                          1/1     Running   0          6h27m   172.16.0.31     master02   <none>           <none>
calico-node-dpnc7                          1/1     Running   1          6h27m   172.16.0.32     master03   <none>           <none>
calico-node-hvt2j                          1/1     Running   1          6h27m   172.16.0.36     node02     <none>           <none>
calico-node-qvtfv                          1/1     Running   1          6h27m   172.16.0.30     master01   <none>           <none>
coredns-8494f9c688-b78ft                   1/1     Running   0          81s     10.244.235.1    master03   <none>           <none>
coredns-8494f9c688-gm2qk                   1/1     Running   0          81s     10.244.140.65   node02     <none>           <none>

[root@master01 /opt/yaml]# kubectl run busybox --image=busybox:1.28 --command -- sleep 3600
pod/busybox created
[root@master01 /opt/yaml]# kubectl get pods -l run=busybox
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     ContainerCreating   0          7s
[root@master01 /opt/yaml]# kubectl get pods -l run=busybox
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          11s
[root@master01 /opt/yaml]# kubectl get pods -l run=busybox
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          13s
[root@master01 /opt/yaml]# POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
[root@master01 /opt/yaml]# kubectl exec -ti $POD_NAME -- nslookup kubernetes
Server:    10.0.0.2
Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local


```

问题

```BASH
/ # nslookup baidu.com
Server:    10.0.0.2
Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'baidu.com'
```

调试DNS

https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-debugging-resolution/

创建一个简单的pod

```BASH
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: tutum/dnsutils:latest   #docker search 一下
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

[root@master01 /opt/yaml]# kubectl apply -f dnsutils.yaml 
pod/dnsutils created
[root@master01 /opt/yaml]# kubectl get pods dnsutils
NAME       READY   STATUS    RESTARTS   AGE
dnsutils   1/1     Running   0          7s
[root@master01 /opt/yaml]# kubectl exec -i -t dnsutils -- nslookup kubernetes.default
Server:		10.0.0.2
Address:	10.0.0.2#53

Name:	kubernetes.default.svc.cluster.local
Address: 10.0.0.1
成功了，试试百度

[root@master01 /opt/yaml]# kubectl exec -i -t dnsutils -- nslookup baidu.com
Server:		10.0.0.2
Address:	10.0.0.2#53

** server can't find baidu.com: SERVFAIL

command terminated with exit code 1

检查DNS
[root@master01 /opt/yaml]# kubectl exec -ti dnsutils -- cat /etc/resolv.conf
nameserver 10.0.0.2
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

```

检查DNS pod 里的错误

```BASH
[root@master01 /opt/yaml]# kubectl logs --namespace=kube-system -l k8s-app=kube-dns

[ERROR] plugin/errors: 2 4088203880720438406.6493476667826366574. HINFO: plugin/loop: no next plugin found
[ERROR] plugin/errors: 2 baidu.com. A: plugin/loop: no next plugin found
[ERROR] plugin/errors: 2 baidu.com. AAAA: plugin/loop: no next plugin found
[ERROR] plugin/errors: 2 baidu.com. AAAA: plugin/loop: no next plugin found
[ERROR] plugin/errors: 2 baidu.com. A: plugin/loop: no next plugin found
[ERROR] plugin/errors: 2 qq.com. A: plugin/loop: no next plugin found

```

检查DNS服务

```BASH
[root@master01 /opt/yaml]# kubectl get svc --namespace=kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.0.0.2     <none>        53/UDP,53/TCP,9153/TCP   8m57s

```

检查DNS端口是否开放

```BASH
[root@master01 /opt/yaml]# kubectl get ep kube-dns --namespace=kube-system
NAME       ENDPOINTS                                                       AGE
kube-dns   10.244.140.67:53,10.244.235.3:53,10.244.140.67:53 + 3 more...   9m46s

```

添加log

```BASH
[root@master01 /opt/yaml]# kubectl -n kube-system edit configmap coredns
...
data:
  Corefile: |
    .:53 {
        log
        errors
        health
...

```

更换配置文件试试

```BASH

```

换coredns:1.6.7 就可以。。。

```BASH
[root@master01 /opt/yaml]#  kubectl exec -i -t dnsutils -- nslookup baidu.com
Server:		10.0.0.2
Address:	10.0.0.2#53

Non-authoritative answer:
Name:	baidu.com
Address: 39.156.69.79
Name:	baidu.com
Address: 220.181.38.148

[root@master01 /opt/yaml]#  kubectl exec -i -t dnsutils -- nslookup kubernetes
Server:		10.0.0.2
Address:	10.0.0.2#53

Name:	kubernetes.default.svc.cluster.local
Address: 10.0.0.1
```

根据上面的结果可以知道这个我因为镜像中配置的问题导致的



## 确认coreDNS

```BASH
[root@master01 ~]# kubectl run busybox --image=busybox:1.28 --command -- sleep 3600

[root@master01 ~]# kubectl get pods -l run=busybox
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          26s


[root@master01 ~]# kubectl exec -it busybox -- nslookup kubernetes
Server:    10.0.0.2
Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local

```



## 测试

### 数据加密测试

验证[加密静态秘密数据](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#verifying-that-data-is-encrypted)的能力

```BASH
创建一个通用密码
kubectl create secret generic kubernetes  \
  --from-literal="mykey=mydata"
  
  ETCDCTL_API=3 etcdctl get \
  --endpoints=https://172.16.0.30:2379 \
  --cacert=/opt/etcd/ssl/ca.pem \
  --cert=/opt/etcd/ssl/etcd.pem \
  --key=/opt/etcd/ssl/etcd-key.pem\
  /registry/secrets/default/kubernetes | hexdump -C
  
  打印kubernetes存储在 etcd中的秘密的十六进制转储：
  [root@master01 ~]# ETCDCTL_API=3 etcdctl get \
  --endpoints=https://172.16.0.30:2379 \
  --cacert=/opt/etcd/ssl/ca.pem \
  --cert=/opt/etcd/ssl/etcd.pem \
  --key=/opt/etcd/ssl/etcd-key.pem\
  /registry/secrets/default/kubernetes | hexdump -C
00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|
00000020  65 74 65 73 0a 6b 38 73  3a 65 6e 63 3a 61 65 73  |etes.k8s:enc:aes|
00000030  63 62 63 3a 76 31 3a 6b  65 79 31 3a dd d3 88 90  |cbc:v1:key1:....|
00000040  5c 52 92 54 f8 22 ce 47  07 60 9b 8b 9f 6e a7 8c  |\R.T.".G.`...n..|
00000050  d6 57 0a be aa d8 d0 e1  85 d1 0a 14 e7 30 87 6d  |.W...........0.m|
00000060  91 8c 82 30 5f 05 3b b5  5f 82 3f dd 2d ef fa 34  |...0_.;._.?.-..4|
00000070  89 e2 49 fd 58 e0 9a bb  b5 3d ef 6a 19 c5 95 48  |..I.X....=.j...H|
00000080  96 35 e8 e7 13 60 f1 43  2e 22 86 b8 95 77 fc a1  |.5...`.C."...w..|
00000090  d1 36 3c f6 29 96 8a e8  39 d0 a2 3d 34 ea d8 8d  |.6<.)...9..=4...|
000000a0  d8 c2 a5 a1 42 48 5e 5d  8a db ae 3c 1c 4c 2f 36  |....BH^]...<.L/6|
000000b0  59 e5 4b 78 fc f6 3f 2d  67 c9 af 65 7c 6b f1 96  |Y.Kx..?-g..e|k..|
000000c0  49 00 48 28 94 a9 55 f3  23 9e cc 08 99 c8 c2 3e  |I.H(..U.#......>|
000000d0  fb a1 b7 f2 81 83 91 93  86 bc 9f e6 6c 50 3f be  |............lP?.|
000000e0  59 eb 22 9a 5b af b4 1d  c2 0b 76 f6 1b 2f f3 61  |Y.".[.....v../.a|
000000f0  98 92 09 f8 ed 7c fc 62  78 62 17 00 48 51 e1 9d  |.....|.bxb..HQ..|
00000100  ac 95 99 55 2b b5 3e 5a  15 31 d2 aa f7 2b 64 fe  |...U+.>Z.1...+d.|
00000110  cb e9 57 ce 7d 70 fc ed  f3 e2 b9 bc 07 6d 06 ae  |..W.}p.......m..|
00000120  a3 ec db 6b 3a cc 8f 77  71 93 29 d9 48 51 ac b0  |...k:..wq.).HQ..|
00000130  2e 72 8a c4 da 6b ef e3  c5 8b f4 1a 0a           |.r...k.......|
0000013d

```

etcd 密钥应以 为前缀`k8s:enc:aescbc:v1:key1`，这表明`aescbc`提供程序用于使用`key1`加密密钥对数据进行加密。

### 部署

验证创建和管理部署的能力

```BASH
[root@master01 ~]# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created


[root@master01 ~]# kubectl get pods -l app=nginx
NAME                     READY   STATUS    RESTARTS   AGE
nginx-6799fc88d8-4vmn2   1/1     Running   0          46s

```

### 端口转发

验证端口转发远程访问程序

```BASH
[root@master01 ~]# kubectl port-forward nginx-6799fc88d8-4vmn2 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80

新建终端
curl --head http://127.0.0.1:8080
```

### 日志

```BASH
[root@master01 ~]kubectl logs nginx-6799fc88d8-4vmn2    
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
2021/07/19 13:31:57 [notice] 1#1: using the "epoll" event method
2021/07/19 13:31:57 [notice] 1#1: nginx/1.21.1
2021/07/19 13:31:57 [notice] 1#1: built by gcc 8.3.0 (Debian 8.3.0-6) 
2021/07/19 13:31:57 [notice] 1#1: OS: Linux 5.13.1-1.el7.elrepo.x86_64
2021/07/19 13:31:57 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2021/07/19 13:31:57 [notice] 1#1: start worker processes
2021/07/19 13:31:57 [notice] 1#1: start worker process 33
2021/07/19 13:31:57 [notice] 1#1: start worker process 34
/docker-entrypoint.sh: Configuration complete; ready for start up

```

### 执行

```BASH
[root@master01 ~]# kubectl exec -it nginx-6799fc88d8-4vmn2 -- nginx -v
nginx version: nginx/1.21.1

```

### 服务

使用NodePort服务公开部署

```BASH
[root@master01 ~]# kubectl expose  deployment nginx --port 80 --type NodePort
service/nginx exposed

[root@master01 ~]# kubectl get svc --all-namespaces 
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP                  2d23h
default       nginx        NodePort    10.0.0.232   <none>        80:30080/TCP             67s
kube-system   kube-dns     ClusterIP   10.0.0.2     <none>        53/UDP,53/TCP,9153/TCP   35h


[root@master01 ~]# curl -I 10.0.0.232:80
HTTP/1.1 200 OK
Server: nginx/1.21.1
Date: Mon, 19 Jul 2021 13:50:11 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 06 Jul 2021 14:59:17 GMT
Connection: keep-alive
ETag: "60e46fc5-264"
Accept-Ranges: bytes

[root@master01 ~]# curl -I 172.16.0.37
HTTP/1.1 200 OK
Server: nginx/1.20.1
Date: Mon, 19 Jul 2021 13:50:23 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Fri, 16 Jul 2021 13:29:52 GMT
Connection: keep-alive
ETag: "60f189d0-264"
Accept-Ranges: bytes

[root@master01 ~]# curl -I 172.16.0.37:80
HTTP/1.1 200 OK
Server: nginx/1.20.1
Date: Mon, 19 Jul 2021 13:50:31 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Fri, 16 Jul 2021 13:29:52 GMT
Connection: keep-alive
ETag: "60f189d0-264"
Accept-Ranges: bytes

```



可以看到服务被映射成功

https://github.com/xyz349925756/kubernetes/blob/main/image-20210719215140366.png



# Harbor

下载：https://github.com/goharbor/harbor/releases

```BASH
[root@master01 ~]# mkdir tls/harbor &&  cd tls/harbor
```

## 证书创建

```BASH
cat > ca-csr.json <<EOF
{
    "CN": "harbor",   
    "hosts": [],     
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "yunnan",
            "L": "kunming",
            "O": "harbor",
            "OU": "S"
        }
    ],
    "ca": {
        "expiry": "87600h" 
    }
}
EOF
#配置CA
cat >ca-config.json<<EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "harbor": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
          
        }
    }
}
EOF

cfssl gencert -initca ca-csr.json |cfssljson -bare ca

#配置harbor服务
cat >harbor-csr.json <<EOF
{
    "CN": "harbor",
    "hosts": [
      "172.16.0.30",
      "master01"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "kunming",
            "L": "kunming",
            "O": "harbor",
            "OU": "ops"
        }
    ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=harbor harbor-csr.json | cfssljson -bare harbor

```

## 解压安装包

```BASH
[root@master01 /server/soft]# tar xf harbor-offline-installer-v2.2.3.tgz -C /opt/
[root@master01 /server/soft]# cd /opt/harbor/
[root@master01 /opt/harbor]# cp harbor.yml.tmpl{,.bak} #备份文件
[root@master01 /opt/harbor]# mv harbor.yml{.tmpl,}  #重命名
需要修改的初始化文件
[root@master01 /opt/harbor]# vim harbor.yml
...
hostname: 172.16.0.30

#http:
# port: 80
# https related config
https:
  # https port for harbor, default is 443
  port: 443
  # The path of cert and key files for nginx
  certificate: /root/tls/harbor/harbor.pem
  private_key: /root/tls/harbor/harbor-key.pem
harbor_admin_password: 12345                                                                                                                              
# The default data volume
data_volume: /data
...

[root@master01 /opt/harbor]# mkdir /data
[root@master01 /opt/harbor]# ./prepare  #检查环境准备
[root@master01 /opt/harbor]# ./install.sh     #安装
✔ ----Harbor has been installed and started successfully.----

这里出现了一个问题80端口被nginx 占用了，修改下nginx 的80端口即可

检查启动情况
[root@master01 /opt/harbor]# docker-compose ps
      Name                     Command                  State                                          Ports                                    
------------------------------------------------------------------------------------------------------------------------------------------------
harbor-core         /harbor/entrypoint.sh            Up (healthy)                                                                               
harbor-db           /docker-entrypoint.sh            Up (healthy)                                                                               
harbor-jobservice   /harbor/entrypoint.sh            Up (healthy)                                                                               
harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514->10514/tcp                                                   
harbor-portal       nginx -g daemon off;             Up (healthy)                                                                               
nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80->8080/tcp,:::80->8080/tcp, 0.0.0.0:443->8443/tcp,:::443->8443/tcp
redis               redis-server /etc/redis.conf     Up (healthy)                                                                               
registry            /home/harbor/entrypoint.sh       Up (healthy)                                                                               
registryctl         /home/harbor/start.sh            Up (healthy)                                                                               

添加开机脚本
[root@master01 /opt/harbor]# echo 'cd /opt/harbor && docker-compose start' >/root/docker-compose.sh
[root@master01 /opt/harbor]# chmod +x /etc/rc.d/rc.local 
[root@master01 /opt/harbor]# echo 'sh /root/docker-compose.sh' >>/etc/rc.d/rc.local 
```

## 客户端访问配置

```BASH
全部访问机器
mkdir -p /etc/docker/certs.d/harbor

[root@master01 /opt/harbor]# cp /root/tls/harbor/harbor.pem /etc/docker/certs.d/harbor/harbor.crt
[root@master01 /opt/harbor]# cp /root/tls/harbor/harbor.pem ~

[root@master01 /opt/harbor]#  for i in master{02,03} node{01,02};do scp /etc/docker/certs.d/harbor/harbor.crt $i:/etc/docker/certs.d/harbor;done
[root@master01 /opt/harbor]#  for i in master{02,03} node{01,02};do scp ~/harbor.pem $i:~;done


[root@master01 /opt/harbor]# sed -i "2i \"insecure-registries\": [\"172.16.0.30\"]," /etc/docker/daemon.json 
注意调整格式！
[root@master01 /opt/harbor]# cat /etc/docker/daemon.json 
{
  "insecure-registries": ["172.16.0.30"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}

[root@master01 /opt/harbor]# for i in master{02,03} node{01,02};do scp /etc/docker/daemon.json $i:/etc/docker;done
daemon.json                                                                 100%  198   135.1KB/s   00:00    
daemon.json                                                                 100%  198   188.7KB/s   00:00    
daemon.json                                                                 100%  198   149.2KB/s   00:00    
daemon.json                                                                 100%  198   223.2KB/s   00:00    

systemctl daemon-reload && systemctl restart docker
```

## 验证

```BASH
[root@master01 /opt/harbor]# docker login 172.16.0.30
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded


#################################################################
[root@node01 ~]# docker login 172.16.0.30
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@node01 ~]# docker pull busybox   
Using default tag: latest
latest: Pulling from library/busybox
b71f96345d44: Pull complete 
Digest: sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest
[root@node01 ~]# docker images
REPOSITORY                  TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/pause            3.4.1     05dd69ba96eb   3 days ago     819kB
nginx                       latest    4cdc5dd7eaad   13 days ago    133MB
busybox                     latest    69593048aa3a   6 weeks ago    1.24MB
calico/node                 v3.19.1   c4d75af7e098   2 months ago   168MB
calico/pod2daemon-flexvol   v3.19.1   5660150975fb   2 months ago   21.7MB
calico/cni                  v3.19.1   5749e8b276f9   2 months ago   146MB
tutum/dnsutils              latest    6cd78a6d3256   6 years ago    200MB
[root@node01 ~]# docker tag 69593048aa3a  172.16.0.30/library/busybox:latest
[root@node01 ~]# docker images | grep busybox
172.16.0.30/library/busybox   latest    69593048aa3a   6 weeks ago    1.24MB
busybox                       latest    69593048aa3a   6 weeks ago    1.24MB
[root@node01 ~]# docker push 172.16.0.30/library/busybox:latest
The push refers to repository [172.16.0.30/library/busybox]
5b8c72934dfc: Pushed 
latest: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527

#################################################################
```

好了，一个服务部署完成。

